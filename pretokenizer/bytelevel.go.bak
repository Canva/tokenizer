package pretokenizer

import (
	"fmt"
	"regexp"
	"strings"

	"github.com/sugarme/tokenizer"
	"github.com/sugarme/tokenizer/normalizer"
	slice "github.com/sugarme/tokenizer/util/slice"
)

// Regular epxression to split string to `word` token
// including prefix whitespace. Contractions and punctuation
// will be split as well.
// Ref.https://regex101.com/r/pf5XJv
const splitRegStr = `'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+`

var splitRE = regexp.MustCompile(splitRegStr)

var BytesChar map[uint8]string = GenerateBytesChar()

var CharBytes map[string]uint8 = func() map[string]uint8 {
	var bc = GenerateBytesChar()
	var cb map[string]uint8 = make(map[string]uint8)
	for b, c := range bc {
		cb[c] = b
	}
	return cb
}()

// BytesChar maps first 0-255 (byte) to first 0-255 `char` in unicode
// Ref. https://en.wikipedia.org/wiki/List_of_Unicode_characters
func GenerateBytesChar() map[uint8]string {
	var bs []uint8 // byte
	var bc map[uint8]string = make(map[uint8]string)

	// Basic latin
	for i := 33; i <= 126; i++ {
		bs = append(bs, uint8(i))
		bc[uint8(i)] = string(uint8(i))
	}

	// latin-1 supplement (excluding `173`)
	for i := 161; i <= 172 && i != 173; i++ {
		bs = append(bs, uint8(i))
		bc[uint8(i)] = string(i)
	}

	// Append `control` byte (0-32) and (127-160) and 173
	// Due to the control byte, first 256 runes will be shifted right 256
	var n = 0
	for i := 0; i <= 255; i++ {
		if !slice.Contain(uint8(i), bs) {
			// if !contain(uint8(i), bs) {
			bs = append(bs, uint8(i))
			bc[uint8(i)] = string(256 + n)
			n += 1
		}
	}

	return bc
}

// ByteLevel provides all the neccessary steps to handle the
// BPE tokenization at byte-level. It takes care of all the required
// processing steps to transform a utf-8 string as needed before and
// after the BPE model does it job.
type ByteLevel struct {
	// whether to add a leading space to the first word.
	// It allows to treat the leading word just as any other words.
	AddPrefixSpace bool

	// Whether the post processing step should trim offsets
	// to avoid including whitespaces.
	TrimOffsets bool
}

// NewByteLevel returns a default ByteLevel with both
// AddPrefixSpace and TrimOffsets set true
func NewByteLevel() *ByteLevel {
	return &ByteLevel{
		AddPrefixSpace: true,
		TrimOffsets:    true,
	}
}

// Alphabet returns set of first 256 unicode `char`
func (bl *ByteLevel) Alphabet() map[string]struct{} {
	var ab = make(map[string]struct{})
	for _, c := range BytesChar {
		ab[c] = struct{}{}
	}

	return ab
}

// SetAddPrefixSpace set `AddPrefixSpace` property
func (bl *ByteLevel) SetAddPrefixSpace(v bool) {
	bl.AddPrefixSpace = v
}

// SetTrimOffsets set `TrimOffsets` property
func (bl *ByteLevel) SetTrimOffsets(v bool) {
	bl.TrimOffsets = v
}

// Implement `PreTokenizer` methods for `ByteLevel`

// type PreTokResult struct {
// Content string
// Offsets tokenizer.Offsets
// }

// PreTokenize transforms all the unicode characters into
// their byte-level counterpart. It also splits the input
// according to the configured regex.
func (bl *ByteLevel) PreTokenize(normalized *normalizer.NormalizedString) (*normalizer.NormalizedString, *[]tokenizer.PreToken) {

	var res []tokenizer.PreToken
	var positions [][]int
	normalizedString := normalized.GetNormalized()

	if bl.AddPrefixSpace && !strings.HasPrefix(normalizedString, " ") {
		normalizedString = fmt.Sprintf(" %v", normalizedString)
		// update normalized with modified string
		newNormalized := normalizer.NewNormalizedFrom(normalizedString)
		normalized = newNormalized
	}

	// positions holds slice of matches' loc
	// (which is 2 element slice loc[0] start - inclusive
	// and loc[1] end - exclusive)
	locs := splitRE.FindAllStringIndex(normalizedString, -1)

	bytes := []byte(normalizedString)

	for _, loc := range locs {
		start := loc[0]
		end := loc[1]

		last := string(bytes[loc[1]-1]) // -1 because of end exclusive

		var next string
		// Exclude last boundary
		if loc[1] == len(bytes) {
			// end -= 1
			next = ""
		} else {
			next = string(bytes[loc[1]])
		}

		// if last `char` is a whitespace, followed by a non-whitespace
		// remove this whitespace
		if last == " " && next != " " {
			end -= 1
		}

		first := string(bytes[loc[0]])
		var prev string
		// Exclude the first `char` of string
		if loc[0] > 0 {
			prev = string(bytes[start-1])
		} else {
			prev = ""
		}

		// If first `char` is not a whitespace but the previous one
		// was, add that whitespace
		if first != " " && prev == " " {
			start -= 1
		}

		positions = append(positions, []int{start, end})

	}

	// setup goroutine to process string concurrently based on `positions`
	type Change struct {
		Char    string
		Changes uint8
	}

	// TODO: implement concurrently split
	var changedToks [][]Change
	var changeMap []normalizer.ChangeMap
	var n = 0

	for i, pos := range positions {
		end := pos[1]
		if i == len(positions) {
			end = pos[1]
		}

		tok := normalizedString[pos[0]:end]

		tokChars := strings.Split(tok, "")

		var changedTok []Change

		for i := 0; i < len(tokChars); i++ {
			size := len(tokChars[i]) // number of bytes for current `char`
			end := n + size

			tokBytes := []byte(normalizedString[n:end])

			n += size

			for idx, b := range tokBytes {
				var change uint8 = 0
				if idx > 0 {
					change = 1
				}

				changedTok = append(changedTok, Change{Char: BytesChar[b], Changes: change})
				var cm normalizer.ChangeMap = normalizer.ChangeMap{
					RuneVal: BytesChar[b],
					Changes: int(change),
				}

				changeMap = append(changeMap, cm)
			}

		} // end loop for one token ('split word')

		changedToks = append(changedToks, changedTok)
	}

	// Update normalizedString
	normalized.Transform(changeMap, 0)

	// Collect splits and their offsets
	var totalLen = 0
	for _, split := range changedToks {
		var length = 0
		var chars []string
		for _, c := range split {
			chars = append(chars, c.Char)
			length += 1
		}
		totalLen += length
		tok := strings.Join(chars, "")
		offsets := []int{totalLen - length, totalLen}
		res = append(res, tokenizer.PreToken{
			Value:   tok,
			Offsets: offsets,
		})
	}

	return normalized, &res
}

// Implement Decoder for `ByteLevel`

// Decode converts any byte-level characters to their unicode couterpart
// before merging everything back into a single string
func (bl *ByteLevel) Decode(tokens []string) string {
	s := strings.Join(tokens, "")
	chars := strings.Split(s, "")

	var bytes []byte

	for _, c := range chars {
		b := CharBytes[c]

		bytes = append(bytes, b)
	}

	return string(bytes)
}

// Implement PostProcessor for ByteLevel
func (bl *ByteLevel) AddedToken(isPair bool) uint {
	return 0
}

func (bl *ByteLevel) Process(encoding, pairEncoding *tokenizer.Encoding, addSpecialTokens bool) *tokenizer.Encoding {

	if !bl.TrimOffsets {
		return tokenizer.DefaultProcess(encoding, pairEncoding, addSpecialTokens)
	}

	var newEncoding *tokenizer.Encoding
	newEncoding = processOffsets(encoding, bl.AddPrefixSpace)

	overflowEncodings := newEncoding.GetOverflowing()
	var newOverflowEncodings []tokenizer.Encoding
	for _, e := range overflowEncodings {
		newEn := processOffsets(&e, bl.AddPrefixSpace)
		newOverflowEncodings = append(newOverflowEncodings, *newEn)
	}
	newEncoding.Overflowing = newOverflowEncodings

	var (
		newPairEncoding         *tokenizer.Encoding
		newOverflowPairEncoding []tokenizer.Encoding
	)

	if pairEncoding != nil {
		newPairEncoding = processOffsets(pairEncoding, bl.AddPrefixSpace)
		for _, en := range newPairEncoding.Overflowing {
			newEn := processOffsets(&en, bl.AddPrefixSpace)
			newOverflowPairEncoding = append(newOverflowPairEncoding, *newEn)
		}
		newPairEncoding.Overflowing = newOverflowPairEncoding
	}

	return tokenizer.DefaultProcess(newEncoding, newPairEncoding, addSpecialTokens)
}

// func processOffsets(isTrimOffsets bool, encoding *tokenizer.Encoding) *tokenizer.Encoding {
func processOffsets(encoding *tokenizer.Encoding, addPrefixSpace bool) *tokenizer.Encoding {

	type Modif struct {
		LeadingSpaces int
		TrailingSpace int
	}

	var modifs []Modif
	var newOffsets [][]int

	toks := encoding.GetTokens()
	for _, tok := range toks {
		var leadingSpaces int = 0
		chars := strings.Split(tok, "")
		fmt.Printf("chars: %v\n", chars)
		for _, c := range chars {
			if c != "Ġ" {
				// if c != BytesChar[' '] && c != " " {
				break
			}
			leadingSpaces += 1
		}

		var trailingSpaces int = 0
		for i := len(chars) - 1; i >= 0; i-- {
			if chars[i] != "Ġ" {
				// if chars[i] != BytesChar[' '] && chars[i] != " " {
				break
			}
			trailingSpaces += 1
		}

		if leadingSpaces > 0 || trailingSpaces > 0 {
			modifs = append(modifs, Modif{
				LeadingSpaces: leadingSpaces,
				TrailingSpace: trailingSpaces,
			})
		}
	}

	for i, m := range modifs {
		var offset0, offset1 int
		offsets := encoding.GetOffsets()[i]
		ld := m.LeadingSpaces
		if m.LeadingSpaces > 0 {
			if i == 0 && addPrefixSpace && m.LeadingSpaces == 1 {
				// If we are processing the first pair of offsets, with `addPrefixSpace`,
				// then we shouldn't remove anything we added. If there are more than one
				// leading spaces though, it means we didn't add them, and they should be
				// removed.
				ld = 0
			}

			offset0 = offsets[0] + ld
			if offset0 > offsets[1] {
				offset0 = offsets[1]
			}
		}

		tl := m.TrailingSpace

		if tl > 0 && offsets[1] >= tl {
			offset1 = offsets[1] - tl
			if offset1 < offset0 {
				offset1 = offset0
			}
		}

		newOffsets = append(newOffsets, []int{offset0, offset1})

	}

	encoding.Offsets = newOffsets

	return encoding
}
